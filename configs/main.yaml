#model:
#arch: mlp
#d_model: 16
#n_layers: 1
#act_fn: relu


optimizer:
  lr: 3.0e-4
  wd: 0.1
  betas: [0.9, 0.95]


data: 
  task: wikitext
  task_params:
    n_ctx: 512
  
train:
  k: 1 # use this to control which model size from the sweep we use
  seed: 42
  device: "cuda:0"
  batch_size: 8
  train_iters: 500
  train_frac: -1
  eval_iters: 50
  eval_every: 50
  save_every: 50


checkpointing:
  dir: "run-1"

wandb:
  project: toy-scaling-3
  group: some
  name: primary
  entity: zhangir-azerbayev

